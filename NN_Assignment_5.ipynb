{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yu3-XoZ74KSL",
        "outputId": "c690dd7c-e2e9-4dce-bbb0-77490ee522cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing set of libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import re\n",
        "#Package for splitting the data\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "#Tokenization\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from keras.utils.np_utils import to_categorical"
      ],
      "metadata": {
        "id": "VN-7iRRn4Sj8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Loading the dataset as a Pandas DataFrame\n",
        "dataset = pd.read_csv('/content/gdrive/My Drive/Sentiment.csv')\n",
        "\n",
        "# Selecting only the necessary columns 'text' and 'sentiment'\n",
        "mask = dataset.columns.isin(['text', 'sentiment'])\n",
        "data = dataset.loc[:, mask]"
      ],
      "metadata": {
        "id": "O6g8iX9q4WU5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keeping only necessary columns\n",
        "data['text'] = data['text'].apply(lambda x: x.lower())\n",
        "data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]', '', x)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vi4aJJv-4YZF",
        "outputId": "fe5e835c-2373-48fd-a94f-0a99a5b2a6b7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-4d9282c66e4b>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['text'] = data['text'].apply(lambda x: x.lower())\n",
            "<ipython-input-4-4d9282c66e4b>:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]', '', x)))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing Re tweets\n",
        "for idx, row in data.iterrows():\n",
        "    row[0] = row[0].replace('rt', ' ')"
      ],
      "metadata": {
        "id": "tDwdbUcw4ZY_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Maximum words is 2000 to tokenize the sentence\n",
        "max_features = 2000\n",
        "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
        "tokenizer.fit_on_texts(data['text'].values)\n",
        "X = tokenizer.texts_to_sequences(data['text'].values)\n"
      ],
      "metadata": {
        "id": "6TCjYDtA4eVB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Padding the feature matrix\n",
        "X = pad_sequences(X)\n",
        "embed_dim = 128\n",
        "#Long short-term memory (LSTM) layer\n",
        "lstm_out = 196"
      ],
      "metadata": {
        "id": "CZ6_rEy74ghv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def createmodel():\n",
        "    model = Sequential() #Sequential Neural Network\n",
        "    #input dimension - 2000 Neurons, output dimension-128 Neurons\n",
        "    model.add(Embedding(max_features, embed_dim,input_length = X.shape[1]))\n",
        "    model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model.add(Dense(3,activation='softmax'))\n",
        "    model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy']) #Compiling the model\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "3rTMQ_8L4sqx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Applying label Encoding on the label matrix\n",
        "labelencoder = LabelEncoder()\n",
        "integer_encoded = labelencoder.fit_transform(data['sentiment'])\n",
        "y = to_categorical(integer_encoded)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,y, test_size = 0.33, random_state = 42)"
      ],
      "metadata": {
        "id": "ASRlduDi4u2U"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32 #Batch size 32\n",
        "model = createmodel()\n",
        "model.fit(X_train, Y_train, epochs = 1, batch_size=batch_size, verbose = 2) #verbose the higher, the more messages\n",
        "score,acc = model.evaluate(X_test,Y_test,verbose=2,batch_size=batch_size) #evaluating the model\n",
        "print(score)\n",
        "print(acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1v-FQRlJ4wYG",
        "outputId": "9cec9300-9c23-4644-82c8-1f22547912fe"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "291/291 - 61s - loss: 0.8264 - accuracy: 0.6436 - 61s/epoch - 209ms/step\n",
            "144/144 - 4s - loss: 0.7842 - accuracy: 0.6551 - 4s/epoch - 25ms/step\n",
            "0.7841821908950806\n",
            "0.6550895571708679\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.metrics_names) #metrics of the model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Du9HAky5caD",
        "outputId": "9d5303f2-825f-49a4-f009-2e49721ec835"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['loss', 'accuracy']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1)Save the model and use the saved model to predict on new text data (ex, “A lot of good things are\n",
        "#happening. We are respected again throughout the world, and that's a great thing.@realDonaldTrump”)\n",
        "\n",
        "#Saving the model\n",
        "model.save('sentimentAnalysis.h5')"
      ],
      "metadata": {
        "id": "oOykGVhb5eS8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing the package for importing the saved model\n",
        "from keras.models import load_model\n",
        "model= load_model('sentimentAnalysis.h5') #loading the saved model"
      ],
      "metadata": {
        "id": "g_LJeVX85hZN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(integer_encoded)\n",
        "print(data['sentiment'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qLA8bIR5jVm",
        "outputId": "bc4090d1-7094-48b7-df3d-f1c36cc0477e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 2 1 ... 2 0 2]\n",
            "0         Neutral\n",
            "1        Positive\n",
            "2         Neutral\n",
            "3        Positive\n",
            "4        Positive\n",
            "           ...   \n",
            "13866    Negative\n",
            "13867    Positive\n",
            "13868    Positive\n",
            "13869    Negative\n",
            "13870    Positive\n",
            "Name: sentiment, Length: 13871, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting on the text data\n",
        "sentence = ['A lot of good things are happening. We are respected again throughout the world, and that is a great thing.@realDonaldTrump']\n",
        "sentence = tokenizer.texts_to_sequences(sentence) # Tokenizing the sentence\n",
        "sentence = pad_sequences(sentence, maxlen=28, dtype='int32', value=0) # Padding the sentence\n",
        "sentiment_probs = model.predict(sentence, batch_size=1, verbose=2)[0] # Predicting the sentence text\n",
        "sentiment = np.argmax(sentiment_probs)\n",
        "\n",
        "print(sentiment_probs)\n",
        "if sentiment == 0:\n",
        "    print(\"Neutral\")\n",
        "elif sentiment < 0:\n",
        "    print(\"Negative\")\n",
        "elif sentiment > 0:\n",
        "    print(\"Positive\")\n",
        "else:\n",
        "    print(\"Cannot be determined\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CX2oSyL45lFG",
        "outputId": "2759bb74-b6a1-44dd-ec6e-72b9c58d10b0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 - 2s - 2s/epoch - 2s/step\n",
            "[0.5035101  0.22521064 0.2712793 ]\n",
            "Neutral\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Apply GridSearchCV on the source code provided in the class\n",
        "\n",
        "#importing Keras classifier\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "#importing Grid search CV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "#initiating model to test performance by applying multiple hyper parameters\n",
        "model = KerasClassifier(build_fn=createmodel,verbose=2)\n",
        "#batch_size\n",
        "batch_size= [10, 20, 40]\n",
        "# no. of epochs\n",
        "epochs = [1, 2]\n",
        "#dictionaries\n",
        "param_grid= {'batch_size':batch_size, 'epochs':epochs}\n",
        "grid  = GridSearchCV(estimator=model, param_grid=param_grid)\n",
        "grid_result= grid.fit(X_train,Y_train) #Fitting the model\n",
        "#summarizing the  results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_)) #best score, best hyper parameters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sQk2NHg5nq4",
        "outputId": "2a57a132-23a6-4fb7-f7bc-212eb47bcd41"
      },
      "execution_count": 16,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-16-cf78e15108c2>:8: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  model = KerasClassifier(build_fn=createmodel,verbose=2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "744/744 - 123s - loss: 0.8243 - accuracy: 0.6421 - 123s/epoch - 166ms/step\n",
            "186/186 - 3s - loss: 0.7384 - accuracy: 0.6772 - 3s/epoch - 17ms/step\n",
            "744/744 - 124s - loss: 0.8213 - accuracy: 0.6468 - 124s/epoch - 166ms/step\n",
            "186/186 - 4s - loss: 0.7669 - accuracy: 0.6584 - 4s/epoch - 24ms/step\n",
            "744/744 - 117s - loss: 0.8253 - accuracy: 0.6423 - 117s/epoch - 157ms/step\n",
            "186/186 - 3s - loss: 0.7630 - accuracy: 0.6805 - 3s/epoch - 16ms/step\n",
            "744/744 - 117s - loss: 0.8316 - accuracy: 0.6402 - 117s/epoch - 157ms/step\n",
            "186/186 - 3s - loss: 0.7527 - accuracy: 0.6706 - 3s/epoch - 17ms/step\n",
            "186/186 - 3s - loss: 0.7860 - accuracy: 0.6652 - 3s/epoch - 17ms/step\n",
            "Epoch 1/2\n",
            "744/744 - 119s - loss: 0.8225 - accuracy: 0.6473 - 119s/epoch - 161ms/step\n",
            "Epoch 2/2\n",
            "744/744 - 113s - loss: 0.6799 - accuracy: 0.7093 - 113s/epoch - 152ms/step\n",
            "186/186 - 4s - loss: 0.7440 - accuracy: 0.6842 - 4s/epoch - 20ms/step\n",
            "Epoch 1/2\n",
            "744/744 - 113s - loss: 0.8296 - accuracy: 0.6492 - 113s/epoch - 152ms/step\n",
            "Epoch 2/2\n",
            "744/744 - 115s - loss: 0.6824 - accuracy: 0.7120 - 115s/epoch - 155ms/step\n",
            "186/186 - 3s - loss: 0.7492 - accuracy: 0.6783 - 3s/epoch - 17ms/step\n",
            "Epoch 1/2\n",
            "744/744 - 117s - loss: 0.8204 - accuracy: 0.6445 - 117s/epoch - 157ms/step\n",
            "Epoch 2/2\n",
            "744/744 - 113s - loss: 0.6734 - accuracy: 0.7156 - 113s/epoch - 152ms/step\n",
            "186/186 - 3s - loss: 0.7448 - accuracy: 0.6853 - 3s/epoch - 16ms/step\n",
            "Epoch 1/2\n",
            "744/744 - 123s - loss: 0.8252 - accuracy: 0.6444 - 123s/epoch - 165ms/step\n",
            "Epoch 2/2\n",
            "744/744 - 115s - loss: 0.6767 - accuracy: 0.7146 - 115s/epoch - 155ms/step\n",
            "186/186 - 3s - loss: 0.7458 - accuracy: 0.6609 - 3s/epoch - 18ms/step\n",
            "Epoch 1/2\n",
            "744/744 - 116s - loss: 0.8237 - accuracy: 0.6469 - 116s/epoch - 156ms/step\n",
            "Epoch 2/2\n",
            "744/744 - 113s - loss: 0.6730 - accuracy: 0.7197 - 113s/epoch - 152ms/step\n",
            "186/186 - 5s - loss: 0.7847 - accuracy: 0.6717 - 5s/epoch - 25ms/step\n",
            "372/372 - 66s - loss: 0.8302 - accuracy: 0.6435 - 66s/epoch - 178ms/step\n",
            "93/93 - 2s - loss: 0.7471 - accuracy: 0.6676 - 2s/epoch - 23ms/step\n",
            "372/372 - 71s - loss: 0.8296 - accuracy: 0.6457 - 71s/epoch - 192ms/step\n",
            "93/93 - 2s - loss: 0.7810 - accuracy: 0.6670 - 2s/epoch - 25ms/step\n",
            "372/372 - 67s - loss: 0.8388 - accuracy: 0.6368 - 67s/epoch - 180ms/step\n",
            "93/93 - 2s - loss: 0.7624 - accuracy: 0.6859 - 2s/epoch - 24ms/step\n",
            "372/372 - 65s - loss: 0.8372 - accuracy: 0.6418 - 65s/epoch - 174ms/step\n",
            "93/93 - 2s - loss: 0.7394 - accuracy: 0.6825 - 2s/epoch - 21ms/step\n",
            "372/372 - 65s - loss: 0.8291 - accuracy: 0.6416 - 65s/epoch - 176ms/step\n",
            "93/93 - 4s - loss: 0.7815 - accuracy: 0.6744 - 4s/epoch - 43ms/step\n",
            "Epoch 1/2\n",
            "372/372 - 70s - loss: 0.8305 - accuracy: 0.6426 - 70s/epoch - 189ms/step\n",
            "Epoch 2/2\n",
            "372/372 - 67s - loss: 0.6841 - accuracy: 0.7096 - 67s/epoch - 181ms/step\n",
            "93/93 - 4s - loss: 0.7456 - accuracy: 0.6751 - 4s/epoch - 40ms/step\n",
            "Epoch 1/2\n",
            "372/372 - 67s - loss: 0.8218 - accuracy: 0.6439 - 67s/epoch - 181ms/step\n",
            "Epoch 2/2\n",
            "372/372 - 61s - loss: 0.6848 - accuracy: 0.7105 - 61s/epoch - 165ms/step\n",
            "93/93 - 2s - loss: 0.7421 - accuracy: 0.6869 - 2s/epoch - 21ms/step\n",
            "Epoch 1/2\n",
            "372/372 - 66s - loss: 0.8271 - accuracy: 0.6442 - 66s/epoch - 178ms/step\n",
            "Epoch 2/2\n",
            "372/372 - 61s - loss: 0.6780 - accuracy: 0.7131 - 61s/epoch - 163ms/step\n",
            "93/93 - 3s - loss: 0.7732 - accuracy: 0.6643 - 3s/epoch - 27ms/step\n",
            "Epoch 1/2\n",
            "372/372 - 70s - loss: 0.8288 - accuracy: 0.6402 - 70s/epoch - 188ms/step\n",
            "Epoch 2/2\n",
            "372/372 - 64s - loss: 0.6761 - accuracy: 0.7106 - 64s/epoch - 172ms/step\n",
            "93/93 - 3s - loss: 0.7380 - accuracy: 0.6776 - 3s/epoch - 36ms/step\n",
            "Epoch 1/2\n",
            "372/372 - 67s - loss: 0.8293 - accuracy: 0.6428 - 67s/epoch - 179ms/step\n",
            "Epoch 2/2\n",
            "372/372 - 64s - loss: 0.6757 - accuracy: 0.7130 - 64s/epoch - 172ms/step\n",
            "93/93 - 2s - loss: 0.7723 - accuracy: 0.6738 - 2s/epoch - 24ms/step\n",
            "186/186 - 43s - loss: 0.8476 - accuracy: 0.6353 - 43s/epoch - 230ms/step\n",
            "47/47 - 2s - loss: 0.7494 - accuracy: 0.6659 - 2s/epoch - 33ms/step\n",
            "186/186 - 46s - loss: 0.8389 - accuracy: 0.6404 - 46s/epoch - 245ms/step\n",
            "47/47 - 2s - loss: 0.7765 - accuracy: 0.6649 - 2s/epoch - 32ms/step\n",
            "186/186 - 43s - loss: 0.8460 - accuracy: 0.6334 - 43s/epoch - 232ms/step\n",
            "47/47 - 2s - loss: 0.7616 - accuracy: 0.6708 - 2s/epoch - 34ms/step\n",
            "186/186 - 44s - loss: 0.8398 - accuracy: 0.6344 - 44s/epoch - 235ms/step\n",
            "47/47 - 2s - loss: 0.7449 - accuracy: 0.6825 - 2s/epoch - 32ms/step\n",
            "186/186 - 45s - loss: 0.8421 - accuracy: 0.6398 - 45s/epoch - 243ms/step\n",
            "47/47 - 1s - loss: 0.7830 - accuracy: 0.6502 - 1s/epoch - 32ms/step\n",
            "Epoch 1/2\n",
            "186/186 - 44s - loss: 0.8467 - accuracy: 0.6372 - 44s/epoch - 237ms/step\n",
            "Epoch 2/2\n",
            "186/186 - 40s - loss: 0.6888 - accuracy: 0.7104 - 40s/epoch - 216ms/step\n",
            "47/47 - 2s - loss: 0.7478 - accuracy: 0.6713 - 2s/epoch - 33ms/step\n",
            "Epoch 1/2\n",
            "186/186 - 43s - loss: 0.8465 - accuracy: 0.6348 - 43s/epoch - 229ms/step\n",
            "Epoch 2/2\n",
            "186/186 - 39s - loss: 0.6903 - accuracy: 0.7016 - 39s/epoch - 210ms/step\n",
            "47/47 - 1s - loss: 0.7403 - accuracy: 0.6869 - 1s/epoch - 31ms/step\n",
            "Epoch 1/2\n",
            "186/186 - 45s - loss: 0.8526 - accuracy: 0.6342 - 45s/epoch - 242ms/step\n",
            "Epoch 2/2\n",
            "186/186 - 41s - loss: 0.6909 - accuracy: 0.7058 - 41s/epoch - 219ms/step\n",
            "47/47 - 2s - loss: 0.7374 - accuracy: 0.6945 - 2s/epoch - 32ms/step\n",
            "Epoch 1/2\n",
            "186/186 - 45s - loss: 0.8495 - accuracy: 0.6316 - 45s/epoch - 241ms/step\n",
            "Epoch 2/2\n",
            "186/186 - 41s - loss: 0.6854 - accuracy: 0.7026 - 41s/epoch - 218ms/step\n",
            "47/47 - 2s - loss: 0.7452 - accuracy: 0.6808 - 2s/epoch - 32ms/step\n",
            "Epoch 1/2\n",
            "186/186 - 42s - loss: 0.8379 - accuracy: 0.6387 - 42s/epoch - 228ms/step\n",
            "Epoch 2/2\n",
            "186/186 - 39s - loss: 0.6750 - accuracy: 0.7108 - 39s/epoch - 209ms/step\n",
            "47/47 - 1s - loss: 0.7828 - accuracy: 0.6690 - 1s/epoch - 30ms/step\n",
            "Epoch 1/2\n",
            "233/233 - 52s - loss: 0.8327 - accuracy: 0.6432 - 52s/epoch - 221ms/step\n",
            "Epoch 2/2\n",
            "233/233 - 49s - loss: 0.6733 - accuracy: 0.7148 - 49s/epoch - 212ms/step\n",
            "Best: 0.680511 using {'batch_size': 40, 'epochs': 2}\n"
          ]
        }
      ]
    }
  ]
}